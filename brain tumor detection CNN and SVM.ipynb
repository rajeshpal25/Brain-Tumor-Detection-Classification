{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage \n",
    "from skimage import filters \n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for image processing with segmentation\n",
    "def image_processing(image):\n",
    "                #Gaussian Blur\n",
    "    blurGaussian = cv2.GaussianBlur(image,(5,5),5)\n",
    "                #mean blur\n",
    "    kernel = np.ones((5,5),np.float32)/25\n",
    "    meanblur = cv2.filter2D(blurGaussian,-1,kernel)\n",
    "                #Median Blur\n",
    "    median = cv2.medianBlur(meanblur,5)\n",
    "          #segmentation of images  using threshhold segmentation\n",
    "    th =155\n",
    "    max_value=255\n",
    "    ret, out1 = cv2.threshold(median,th,max_value,cv2.THRESH_BINARY)\n",
    "        #contours\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(out1)\n",
    "    plt.tick_params(axis='both', which='both',top=False, bottom=False, left=False, right=False,labelbottom=False, labeltop=False, labelleft=False, labelright=False)\n",
    "    plt.title('Cropped Image')\n",
    "    plt.show()\n",
    "                    #segmenatation of images using historigram\n",
    "    histimg=plt.hist(median.ravel(),256,[0,256])\n",
    "    plt.show()\n",
    "\n",
    "    iar = np.asarray(median)\n",
    "    print(iar)\n",
    "    iar.max()\n",
    "    # 'contrast',‘dissimilarity’, ‘homogeneity’, ‘energy’, ‘correlation’, ‘ASM’\n",
    "    glcm=g = greycomatrix(iar, [1,2], [0, np.pi/2], levels=260,normed=True, symmetric=True)\n",
    "\n",
    "    contrast = greycoprops(glcm, 'contrast')\n",
    "    print(\"the contrast of the images is \",contrast)\n",
    "    dissimilarity = greycoprops(glcm, 'dissimilarity')\n",
    "    print(\"the dissimilarity of the images is \",dissimilarity)\n",
    "    homogeneity = greycoprops(glcm, 'homogeneity')\n",
    "    print(\"the homogeneity of the images is \",homogeneity)\n",
    "    energy = greycoprops(glcm, 'energy')\n",
    "    print(\"the energy of the images is \",energy)\n",
    "    correlation = greycoprops(glcm, 'correlation')\n",
    "    print(\"the correlation of the images is \",energy)\n",
    "    ASM = greycoprops(glcm, 'ASM')\n",
    "    print(\"the ASM of the images is \",ASM)\n",
    "\n",
    "\n",
    "    plt.plot(contrast,label=\"contrast\")\n",
    "    plt.plot(dissimilarity,label=\"dissimilarity\")\n",
    "    plt.plot(homogeneity,label=\"homogeneity\")\n",
    "    plt.plot(energy,label=\"energy\")\n",
    "    plt.plot(correlation,label=\"correlation\")\n",
    "    plt.plot(ASM,label=\"ASM\")\n",
    "    plt.title(\"GSLM MATRIX\")\n",
    "    plt.xlabel(\"OFFSITE\")\n",
    "    plt.ylabel(\"FEATURES\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return out1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_img=int(input(\"enter the re-sized images\"))\n",
    "IMG_DIR=\"G:\\\\brainMydataSet\"\n",
    "CATEGORIES = [\"no\",\"yes\"]\n",
    "X=[]\n",
    "Y=[]\n",
    "def create_data_set():\n",
    "    for categories in CATEGORIES:\n",
    "        path = os.path.join(IMG_DIR,categories)\n",
    "        #this function is used to cencatenate the path with sub folder\n",
    "        #class_num can classified categories and marked it as 0 and 1 form.\n",
    "        class_num = CATEGORIES.index(categories) \n",
    "        for img in os.listdir(path):\n",
    "             # load the image\\n\",\n",
    "            img_array =cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "            resized_img = cv2.resize(img_array,(size_img,size_img),interpolation=cv2.INTER_CUBIC)\n",
    "            processed_img=image_processing(resized_img)\n",
    "            # normalize values\n",
    "            normal_img = processed_img/255\n",
    "            # convert image to numpy array and append it to X\n",
    "            X.append(normal_img)\n",
    "            # append a value of 1 to the target array if the image is in the folder named 'yes', otherwise append 0.\n",
    "            #from classnum\n",
    "            Y.append(class_num)\n",
    "    x=np.array(X)\n",
    "    y=np.array(Y)\n",
    "    \n",
    "    print(f'Number of examples is: {len(x)}')\n",
    "    print(f'X shape is: {x.shape}')\n",
    "    print(f'y shape is: {y.shape}')\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_data_set()\n",
    "print(np.ndim(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle two lists with same order \n",
    "# Using zip() + * operator + shuffle() \n",
    "temp = list(zip(X,Y)) \n",
    "random.shuffle(temp) \n",
    "X,y= zip(*temp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n",
    "np.shape(X)\n",
    "print(np.ndim(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2):\n",
    "       \n",
    "    \"\"\"\n",
    "    Splits data into training, development and test sets.\n",
    "    Arguments:\n",
    "        X: A numpy array with shape = (#_examples, image_width, image_height, #_channels)\n",
    "        y: A numpy array with shape = (#_examples, 1)\n",
    "    Returns:\n",
    "        X_train: A numpy array with shape = (#_train_examples, image_width, image_height, #_channels)\n",
    "        y_train: A numpy array with shape = (#_train_examples, 1)\n",
    "        X_val: A numpy array with shape = (#_val_examples, image_width, image_height, #_channels)\n",
    "        y_val: A numpy array with shape = (#_val_examples, 1)\n",
    "        X_test: A numpy array with shape = (#_test_examples, image_width, image_height, #_channels)\n",
    "        y_test: A numpy array with shape = (#_test_examples, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=test_size)\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"number of training examples = \" + str(np.shape(X_train)[0]))\n",
    "print (\"number of development examples = \" + str(np.shape(X_val)[0]))\n",
    "print (\"number of test examples = \" + str(np.shape(X_test)[0]))\n",
    "print (\"X_train shape: \" + str(np.shape(X_train)))\n",
    "print (\"Y_train shape: \" + str(np.shape(y_train)))\n",
    "print (\"X_val (dev) shape: \" + str(np.shape(X_val)))\n",
    "print (\"Y_val (dev) shape: \" + str(np.shape(y_val)))\n",
    "print (\"X_test shape: \" + str(np.shape(X_test)))\n",
    "print (\"Y_test shape: \" + str(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.ndim(X_train))\n",
    "print(np.ndim(X_test))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return format\"{h}:{m}:{round(s,1)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(y_true, prob):\n",
    "    # convert the vector of probabilities to a target vector\n",
    "    y_pred = np.where(prob > 0.5, 1, 0)\n",
    "    \n",
    "    score = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a convolutional neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"\n",
    "    Arugments:\n",
    "        input_shape: A tuple representing the shape of the input of the model. shape=(image_width, image_height, #_channels)\n",
    "    Returns:\n",
    "        model: A Model object.\n",
    "    \"\"\"\n",
    "    # Define the input placeholder as a tensor with shape input_shape. \n",
    "    X_input = tf.keras.Input(input_shape)# shape=(?, 128, 128, 1)\n",
    "    print(X_input.shape)\n",
    "    \n",
    "    # Zero-Padding: pads the border of X_input with zeroes\n",
    "    X = ZeroPadding2D((2, 2))(X_input) # shape=(?, 244, 244, 3)\n",
    "    print(X.shape)\n",
    "    # CONV -> BN -> RELU Block applied to X\n",
    "    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n",
    "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n",
    "    X = Activation('relu')(X) # shape=(?, 238, 238, 32)\n",
    "    print(X.shape)\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((4, 4), name='max_pool0')(X) # shape=(?, 59, 59, 32) \n",
    "    print(X.shape)\n",
    "    # MAXPOOL\n",
    "    X = MaxPooling2D((4, 4), name='max_pool1')(X) # shape=(?, 14, 14, 32)\n",
    "    print(X.shape)\n",
    "    # FLATTEN X \n",
    "    X = Flatten()(X)# shape=(?, 6272)\n",
    "    print(X.shape)\n",
    "    # FULLYCONNECTED\n",
    "    X = Dense(1, activation='sigmoid', name='fc')(X) # shape=(?, 1)\n",
    "    \n",
    "    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    model = Model(inputs = X_input, outputs = X, name='BrainDetectionModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (size_img, size_img,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.ndim(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val= np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train,(177,size_img, size_img,1))\n",
    "X_test = np.reshape(X_test,(38,size_img, size_img,1))\n",
    "X_val = np.reshape(X_val,(38,size_img, size_img,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fit model on training data\")\n",
    "modelhistory = model.fit(x=X_train, y=y_train, batch_size=32, epochs=7, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelhistory.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance\n",
    "acc = modelhistory.history['accuracy']\n",
    "val_acc = modelhistory.history['val_accuracy']\n",
    "loss = modelhistory.history['loss']\n",
    "val_loss = modelhistory.history['val_loss']\n",
    "epochs_range = range(1, len(modelhistory.epoch) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Set')\n",
    "plt.plot(epochs_range, val_acc, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Set')\n",
    "plt.plot(epochs_range, val_loss, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save model ..\n",
    "model.save('BrainTumorDetection.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath1=\"G:\\\\brainMydataSet\\\\yes\\\\Y104.jpg\"\n",
    "filepath2=\"G:\\\\brainMydataSet\\\\no\\\\25 no.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "CATEGORIES = [\"no\",\"yes\"]\n",
    "\n",
    "\n",
    "def prepare(filepath):\n",
    "    IMG_SIZE = 128  # 50 in txt-based\n",
    "    img_array = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "    return new_array.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(\"BrainTumorDetection.model\")\n",
    "\n",
    "prediction = model.predict([prepare(filepath1)])\n",
    "print(prediction)\n",
    "# will be a list in a list.\n",
    "cate=(int(str(float(prediction)).split('.')[0]))\n",
    "print(cate)\n",
    "print(CATEGORIES[cate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict([prepare(filepath2)])  # will be a list in a list.\n",
    "print(prediction[0][0])\n",
    "type(prediction)\n",
    "np.ndim(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Svm model\n",
    "from sklearn import svm\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "classifier.fit(X[:176], y[:176])\n",
    "predictions = classifier.predict(X[176:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "# Linear Kernel\n",
    "#Train the model using the training sets\n",
    "print(clf.fit(d2_train_dataset, y_train))\n",
    "#Predict the response for test dataset\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train,(177,size_img, size_img,1))\n",
    "X_test = np.reshape(X_test,(177,size_img, size_img,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples, nx, ny = np.shape(X_train)\n",
    "ntsamples, ntx, nty = np.shape(X_test)\n",
    "X_train = np.reshape(X_train,(nsamples,nx*ny))\n",
    "X_test = np.reshape(X_test,(ntsamples,ntx*nty))\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.38458940e-02  3.90365912e+01  9.85222151e-01  1.32078054e+03\n",
      "   5.56855287e-01  5.95298862e+01  5.24408557e+03  6.20515599e+00\n",
      "   8.20527642e+00  9.48254082e-04  2.82972083e+00 -4.82824630e-01\n",
      "   9.97299387e-01]\n",
      " [ 6.04977715e-02  6.61910773e+01  9.74955505e-01  1.32146960e+03\n",
      "   4.98364775e-01  5.96245283e+01  5.21968733e+03  6.22110494e+00\n",
      "   8.53593482e+00  7.71744015e-04  3.18530157e+00 -4.23288702e-01\n",
      "   9.94875630e-01]\n",
      " [ 6.38321570e-02  3.03696873e+01  9.88503116e-01  1.32077907e+03\n",
      "   5.53532560e-01  5.95299367e+01  5.25274657e+03  6.20343372e+00\n",
      "   8.19894006e+00  9.39697872e-04  2.81247410e+00 -4.84013625e-01\n",
      "   9.97334103e-01]\n",
      " [ 6.04988698e-02  6.58855680e+01  9.75071100e-01  1.32146960e+03\n",
      "   4.98530070e-01  5.96245283e+01  5.21999284e+03  6.22212037e+00\n",
      "   8.53706380e+00  7.71363299e-04  3.19191215e+00 -4.23080163e-01\n",
      "   9.94864016e-01]]\n",
      "[ 6.21686731e-02  5.03707309e+01  9.80937968e-01  1.32112470e+03\n",
      "  5.26820673e-01  5.95772199e+01  5.23412808e+03  6.21295376e+00\n",
      "  8.36930377e+00  8.57764817e-04  3.00485216e+00 -4.53301780e-01\n",
      "  9.96093284e-01]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import mahotas as mt\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "IMG_DIR=\"G:\\\\brainMydataSet\"\n",
    "CATEGORIES = [\"no\",\"yes\"]\n",
    "\n",
    "for categories in CATEGORIES:\n",
    "    path = os.path.join(IMG_DIR,categories) #this function is used to cencatenate the path with sub folder\n",
    "    for img in os.listdir(path):\n",
    "        img_array =cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "        break\n",
    "    break\n",
    "        # calculate haralick texture features for 4 types of adjacency\n",
    "textures = mt.features.haralick(img_array)\n",
    "print(textures)\n",
    "        # take the mean of it and return it\n",
    "ht_mean = textures.mean(axis=0)\n",
    "print(ht_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=tf.keras.Input(input_shape)))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val= np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
